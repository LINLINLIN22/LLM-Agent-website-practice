標題: Google AI 開發手勢追蹤技術　助突破手語溝通障礙
原文: https://www.ectimes.org.tw/2019/08/google-ai-%e9%96%8b%e7%99%bc%e6%89%8b%e5%8b%a2%e8%bf%bd%e8%b9%a4%e6%8a%80%e8%a1%93%e3%80%80%e5%8a%a9%e7%aa%81%e7%a0%b4%e6%89%8b%e8%aa%9e%e6%ba%9d%e9%80%9a%e9%9a%9c%e7%a4%99/
根據內政部統計至2018年底，台灣聽覺機能障礙者逾12萬人，聽障者與大眾的語言隔閡不容忽視，19日Google AI 實驗室發表一項手部動態即時追蹤技術，試圖利用簡化資料量的方式，降低手語轉譯的技術難度語電腦耗能，未來大眾有可能用智慧型手機就看懂手語語言。

￭ Google AI 實驗室針對手部規劃21個定位點，動態追蹤手部動作。（截圖自／Google AI Blog）

據外媒報導手語資料辨識的困難處在於，手勢變換通常很快速，具備快速反應捕捉手勢定位的技術發展仍然有限，且各式手語系統中皆含有大量雙手並用的表現形式，可能會因兩手交疊遮住手指動作，或在自然環境中手勢變換的自然顫抖，提高手勢辨識困難度；新創公司SignAll所開發的手語翻譯系統便採用多鏡頭多角度拍攝捕捉手勢相對位置，然而這對影像辨識系統而言亦有其困難度，此外此作法也將大幅提升資料量，增加處理器耗能。

Google AI實驗室從簡化開始，不急著一次追蹤到位手部各處位置。而從相對容易追蹤的手掌開始，因手掌的輪廓較方正，且動作變化較小；定位好的手掌便可延伸至分析已設定好的21個座標上的各手指關節，分析手掌至指關節再到指尖的距離及手掌大小推斷手勢意義。

為了讓系統能更加準確的定位到21個點，Google AI 利用了大量人力為不同角度、光源的3萬張手勢照片人工定位出21個點，作為訓練演算法模型的資料集，期望透過機器學習提高系統辨識度；目前Google還沒把此項研究成果應用在商業產品上，也提供開放源碼供研究人員使用。

目前Google AI實驗室的手語辨識技術發展進程，較適用於辨識由基本拼音構成的「手語拼寫」（Fingerspelling）系統，對於跨手語系統及較複雜的語言轉換仍有許多困難需克服。語言是約定俗成的工具，如口語一般不同地區使用不同系統，從文法結構、口語語序不盡相同，至各地文化脈絡產生的獨特表達形式，在不同語系之間轉譯的系統需賴在內容上深入社會背景，技術上持續維護更新。

