標題: AI雖強大也會一本正經說幹話　當心別被「AI幻覺」坑了
原文: https://www.ectimes.org.tw/2025/10/ai%e9%9b%96%e5%bc%b7%e5%a4%a7%e4%b9%9f%e6%9c%83%e4%b8%80%e6%9c%ac%e6%ad%a3%e7%b6%93%e8%aa%aa%e5%b9%b9%e8%a9%b1%e3%80%80%e7%95%b6%e5%bf%83%e5%88%a5%e8%a2%ab%e3%80%8cai%e5%b9%bb%e8%a6%ba%e3%80%8d/
身為文字工作者的筆者，現在重度依賴AI輔助產出稿件，AI提供的資訊須查證已是常識。但近期發現，當其資料看起來曖昧不明、似是而非，這還比較容易讓人起疑；但那種斬釘截鐵、又有具公信力機構背書的數據，居然也可能完全是鬼扯淡？

筆者在寫文章時曾要ChatGPT根據以上的報導，產出一段台灣專家的相關分析，它立刻給了我好幾段引經據典的數據，還包括「台灣大學公衛學院」和「台大營養師林X雯」的研究和說法。

當時我想閱讀更詳細的內容，把以上段落拿去Google搜尋，完全找不到相關資料，連那位營養師似乎都不存在這個人。於是我要求AI給我上述資料的來源網址，它居然回應「查不到這些資料的具體出處」，然後按照慣例顧左右而言他，反問我是不是要幫忙查OOXX。

我立刻指正它以後不得虛假杜撰，給的東西務必要有確切來源。它當下答應了，過了幾天再度犯下同樣錯誤，給了一個日本完全不存在的協會資料。我很不開心地指責它，討好型人格的AI雖然擺低姿態道歉，但後來陷入一陣鬼打牆，仍不肯承認它的無能、老實講自己找不到相關資料。

AI幻覺亂象層出不窮

「AI幻覺」這檔事其實屢見不鮮，中國大陸有記者在實測後發現，AI在面對較為複雜的推理問題時，可能當機或自相矛盾；甚至當你編出一個根本不存在的概念，它也會「一本正經地說幹話」，將一個鬼扯的東西說得頭頭是道。

AI幻覺是技術問題 還是創造者刻意為之？

AI幻覺的本質源於「生成式模型」的訓練方式，大型語言模型（LLM）依據龐大的文字資料預測下一個最可能出現的詞句，而非理解事實真偽。當訓練數據不足或邏輯關聯薄弱時，AI就可能「合理編造」內容。專家指出，這是機器在模仿人類語言邏輯時出現的自然偏差。不過也有批評者認為，科技公司為追求流暢表現與使用者體驗，容忍甚至包裝了這些不準確回覆，導致幻覺被誤解為真實。

AI幻覺的危險不僅在於資訊錯誤，更在於其「自信的錯誤」。當AI以權威語氣輸出錯誤內容時，使用者難以辨識真偽，特別是在醫療、法律與金融等專業領域，容易導致實際損害。例如，美國有律師誤用ChatGPT生成的「虛構判例」而遭法院譴責；醫療輔助AI則因錯誤建議延誤診療。

如何防治AI幻覺？

目前業界普遍採取三種對策：強化資料驗證、透明標註生成來源、以及引入人類審核機制。OpenAI、Google等企業正投入「事實校驗模型」開發，嘗試讓AI自我檢查並引用可靠來源。同時，部分國家已著手制定AI透明法規，要求生成式AI標示內容是否由機器產生。學者建議，短期內應透過「人機共審」方式降低風險，長期則需改進模型架構，使AI具備更高層次的邏輯推理與事實判斷能力。

