標題: Meta開源多模態AI模型ImageBind　將更近一步邁向人類
原文: https://www.ectimes.org.tw/2023/05/meta%e9%96%8b%e6%ba%90%e5%a4%9a%e6%a8%a1%e6%85%8bai%e6%a8%a1%e5%9e%8bimagebind%e3%80%80%e5%b0%87%e6%9b%b4%e8%bf%91%e4%b8%80%e6%ad%a5%e9%82%81%e5%90%91%e4%ba%ba%e9%a1%9e/
社群網路服務公司Meta宣布開發多模態AI模型ImageBind，為首個整合六種不同感知形式，包括照片/影片、文字、聲音、深度、熱量與慣性測量單元（IMU），以接近人類感知環境的方式，來預測數據之間關連的AI模型，未來或許可望創造身臨其境的多種感官體驗。

儘管目前ImageBind仍是尚未有實際應用的初階框架，不過Meta透露此模型預期成果為模擬人類感知，若搭配虛擬實境設備使用，還能生成更多的感官效果，直接在使用者的所處位置，產生聲音、視覺影像以及物理環境中的動作感受，例如當你想要來一趟海上之旅，ImageBind不僅能夠生成海浪聲音，還能讓你感受涼爽海風，更可以讓你置身於搖晃的甲板。

由此可見，ImageBind系統整合了6大感知形式，讓機器能夠像人類一樣，全面分析不同類型的資料，具備以文字造就影像與聲音，或以聲音來產生影像的生成能力，未來若有機會，還能讓使用者僅需輸入一段文字或影片等，即可生成一個完整且複雜的場景。

￭ 整合6大感知形式的AI模型ImageBind，帶領人們感受豐富感官體驗。（示意圖來源／Pexels）

Meta表示，期望未來可以增加其他感官數據至AI模型中，包括觸覺、嗅覺、語音和腦部Fmri訊號等，ImageBind可應用的場域也將大幅增加，遊戲開發人員可藉此減少開發過程中的繁瑣設計工作。同時，此項系統工具或許也有機會幫助視覺或聽覺障礙者，協助他們透過各種媒介的輔助，來感知周遭環境，使人類學習模式向前一大步。

