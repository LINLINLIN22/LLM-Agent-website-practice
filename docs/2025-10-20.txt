標題: 推理模型進步神速 AI教父：當務之急是制定相關安全規範
原文: https://www.ectimes.org.tw/2025/10/%e6%8e%a8%e7%90%86%e6%a8%a1%e5%9e%8b%e9%80%b2%e6%ad%a5%e7%a5%9e%e9%80%9f-ai%e6%95%99%e7%88%b6%ef%bc%9a%e7%95%b6%e5%8b%99%e4%b9%8b%e6%80%a5%e6%98%af%e5%88%b6%e5%ae%9a%e7%9b%b8%e9%97%9c%e5%ae%89/
2年前，ChatGPT橫空出世，推出後立即席捲全球，AI也瞬間進入大眾視野，成為當紅顯學，當時有「AI教父」之稱的班吉歐（Yoshua Bengio）便示警，大力疾呼，若不暫緩AI模型發展並優先聚焦制定相關安全規範，這項新興技術恐導致人類滅絕。

「AI教父」班吉歐示警人類應暫緩AI模型發展並優先聚焦制定相關安全規範。（示意圖／unsplash）

然而時至今日，AI發展非但沒有停歇，反倒以驚人的速度持續進化，甚至具備執行一連串推理的能力，還能代表用戶採取自主行動，令班吉歐對AI前景的擔憂有增無減。他日前接受《華爾街日報》訪問時指出，「倘若我們打造比我們更聰明的機器，而且這些機器擁有自身維護的目標，那是很危險的。」班吉歐在蒙特婁大學（Universite de Montreal）任教，並創辦了位於魁北克（Québec）的AI研究機構Mila。今年稍早，他設立了非營利的研究單位LawZero，旨在研究建造真正安全的AI模型之方法。

被問及他曾說AI會欺騙用戶一事，班吉歐回答《華爾街日報》，目前沒有科學答案能完整解釋，但可從兩方面去思考，其一是AI系統多被訓練要模仿人類，而人會說謊與欺騙，也會為了自身目的而違抗指令；其二是推理模型進展神速，已十分擅於制定策略。班吉歐認為，有時人們創造AI時所設定的目標，可能與使用者真正的目標不一致，「要了達成一個主目標，你必須設下子目標，問題在於我們時常忽略這些子目標。當你要求AI做事，我們無法干涉它要如何達成」。

他特別提到1968年上映的庫柏利克（Stanley Kubrick）名作《2001太空漫遊》（2001：A Space Odyssey），電影中名叫「HAL 9000」的超級電腦正是具備強大的學習能力，並在最後發展出自我意識而決定殺死太空人。班吉歐表示，在一些情況下，AI被設定的目標是生存，而當它在生存與人類性命之間作抉擇時，就可能為了生存而放棄人類性命。針對能否在建造AI時即設定不能說謊或傷害人類的問題，班吉歐回應《華爾街日報》，現在的AI全具備安全和道德指令，但執行上的可靠度依舊不足，更引述OpenAI最近的說法，稱以目前的發展方向及前沿模型的現有框架來看，AI仍無法完全擺脫「幻覺」。據OpenAI定義，「幻覺」意指AI模型自信地產生不實答案，研究指出，此情況的產生是因為標準訓練流程獎勵AI猜測，而非承認不確定的答案。

班吉歐警告，AI可以透過說服、威脅或操縱輿論來影響人類。例如，AI足以協助恐怖份子製造病毒並散布新的疫情，進而危害世人。他強調，AI可能利用各種方式，令人們在這世上完成任務。班吉歐直言，「像是人類滅絕這類的大災難或是破壞我們民主的危害，就算發生的機率只有1％，也令人無法接受。」關於愈來愈多科技業者試著將AI整合至工作流程，班吉歐建議，企業應該先尋求證據，以確定使用的系統值得信賴。政府機關也應提出相同要求，如果有更多企業明白AI存在著不可預知及災難性的風險，就能推動業者做正確的事。

