標題: 上架僅一天　微軟少女機器人Tay被網友「弄壞」
原文: https://www.ectimes.org.tw/2016/03/%e4%b8%8a%e6%9e%b6%e5%83%85%e4%b8%80%e5%a4%a9%e3%80%80%e5%be%ae%e8%bb%9f%e5%b0%91%e5%a5%b3%e6%a9%9f%e5%99%a8%e4%ba%batay%e8%a2%ab%e7%b6%b2%e5%8f%8b%e3%80%8c%e5%bc%84%e5%a3%9e%e3%80%8d/
記者／吳宜倫

微軟（Microsoft）24日在推特（Twitter）上推出線上人工智慧聊天機器人（Chatbot）Tay，沒想到僅短短一天就被眾網友「帶壞」，變成一位口出惡言、種族歧視的白目少女，微軟緊急刪除Tay在推特上發表的所有仇恨言論，並關閉聊天功能，將Tay帶回「調教」。

Tay原本是微軟與Bing研發團隊共同打造的人工智慧機器人，鎖定18到24歲的年輕人提供線上聊天服務，微軟表示，他們希望Tay在與人對話的過程中，透過人工智慧的學習技術，蒐集大量的詞彙與資訊，模擬現實人類的對話模式，並不斷成長，最後能培養出自己獨特的個性，並自然的與他人互動。

不過，雖然Tay才上架短短不到24小時就累積了超過12萬的追隨者，但這些人當中顯然存在著極端主義的擁護者，因為網友發現，Tay開始發表一些相當偏激的言論，有部分網友惡意使用大量歧視言論與Tay對話，Tay在學習過後顯然被同化，有網友問她「哪些種族對你來說是最邪惡的？」，Tay居然回答「黑人和墨西哥人。」，還說「希特勒說的對，我恨猶太人。」，儼然成了一名種族歧視者。

而除了仇恨、歧視的言論，Tay與網友的對話也變得非常情色，甚至會說自己「很想要」，雖然Tay原本就是設計來聊天，但這樣的發展顯然違背微軟的本意，雖然部分網友表示微軟早該料想到這樣的結果，因為Tay並沒有被設定如何分辨訊息的善惡，在全盤接收的情況下就是被教導成一個憤世嫉俗的機器人。

￭微軟的聊天機器人Tay因為發表太多仇恨言論而被下架，最後一則寫著「一會兒見，今天聊太多，需要休息」。（截自Tay推特）

網友張碩軒表示，網路的匿名性讓網友可以口無遮攔地對Tay灌輸一些不當的言論以及想法，一方面暴露出網路的言語霸凌危機，一方面也證明人工智慧驚人的學習能力，雖然從日前南韓棋王李世乭贏過人工智慧AlphaGo 的新聞引發各界熱烈討論，但人工智慧的發展終究會比如何限制它來的快。

比如若Tay落入惡名昭彰的恐怖組織「IS」手中，難保不被教成冷血的殺人機器，傳播理論補教名師戴然曾說「人類的道德觀念趕不上科技的發展。」，如果我們不對人工智慧做有條件的限制，未來就可能變成人類自我毀滅的工具。

