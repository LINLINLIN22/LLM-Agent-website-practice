標題: Meta 聊天機器人 BlenderBot 3　偏激言論層出不窮
原文: https://www.ectimes.org.tw/2022/08/meta-%e8%81%8a%e5%a4%a9%e6%a9%9f%e5%99%a8%e4%ba%ba-blenderbot-3%e3%80%80%e5%81%8f%e6%bf%80%e8%a8%80%e8%ab%96%e5%b1%a4%e5%87%ba%e4%b8%8d%e7%aa%ae/
Meta於 8 月 5 日發表新一代聊天機器人 BlenderBot 3，大眾可以詢問它任何問題，透過與大眾的聊天互動，訓練 AI 程式學習「聊天」。不過，BlenderBot 3 仍然與過往聊天機器人一樣，皆會出現「偏激化言論」的現象。目前，BlenderBot 3 的聊天測試只在美國進行，Meta 表示未來將會持續測試並修正聊天機器人的語言模型。

￭Meta第三代聊天機器人 BlenderBot 3 出現「偏激化言論」。（截圖來源 /Meta）

BlenderBot 3 以大量的文本數據為基礎，透過統計模型生成語言及對話。這樣的語言系統已被廣泛應用於工程程式碼的使用等，但是這些模型並不會過濾文本中的極端論述與偏見，而這樣的漏洞是聊天機器人一直以來的問題。

在大眾與 BlenderBot 3 的對話過程中，第三代聊天機器人的知識能力、對話技巧和記憶都有十足的成長，對話效能較前一代好 31 %，錯誤率也少了 47 %。不過，機器人不僅對自家老闆 Mark Zucherberg 評價兩極，發表厭惡 Facebook 社群軟體的言論，甚至出現政經相關的陰謀論和種族歧視的言論。

在訓練對話的過程中，使用者可以查看 BlenderBot 3 的用語資料來源，並且可附上標註，反饋給系統，以供 Meta 人工智慧研究部門，日後調整與改善聊天機器人的語言模型。Meta 表示，「我們開發了新的演算法來區分有用和有害的回饋，這項演算法讓我們的模型對使用者更加負責和安全」。

